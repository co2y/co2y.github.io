<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="big data,survey," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="本文主要对当前主流的大数据处理平台做一个简介，包括它们的发展史、整体结构、相互之间关系和使用场景，属于科普文章。">
<meta name="keywords" content="big data,survey">
<meta property="og:type" content="article">
<meta property="og:title" content="大数据平台介绍">
<meta property="og:url" content="http://co2y.github.io/2017/03/09/big-data-platform-survey/index.html">
<meta property="og:site_name" content="Co2y&#39;s Blog">
<meta property="og:description" content="本文主要对当前主流的大数据处理平台做一个简介，包括它们的发展史、整体结构、相互之间关系和使用场景，属于科普文章。">
<meta property="og:image" content="http://co2y.github.io/img/hadoop-hdfs.jpg">
<meta property="og:image" content="http://co2y.github.io/img/hadoop-yarn.png">
<meta property="og:image" content="http://co2y.github.io/img/hadoop-mapreduce.png">
<meta property="og:image" content="http://co2y.github.io/img/hadoop-tez.jpg">
<meta property="og:image" content="http://co2y.github.io/img/hbase-logical.png">
<meta property="og:image" content="http://co2y.github.io/img/hbase-physical.png">
<meta property="og:image" content="http://co2y.github.io/img/hive-cluster.png">
<meta property="og:image" content="http://co2y.github.io/img/storm-structure.png">
<meta property="og:image" content="http://co2y.github.io/img/spark-deploy.png">
<meta property="og:image" content="http://co2y.github.io/img/FlinkOnYarn.svg">
<meta property="og:updated_time" content="2017-03-17T07:09:41.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="大数据平台介绍">
<meta name="twitter:description" content="本文主要对当前主流的大数据处理平台做一个简介，包括它们的发展史、整体结构、相互之间关系和使用场景，属于科普文章。">
<meta name="twitter:image" content="http://co2y.github.io/img/hadoop-hdfs.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://co2y.github.io/2017/03/09/big-data-platform-survey/"/>





  <title> 大数据平台介绍 | Co2y's Blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Co2y's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">solver</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://co2y.github.io/2017/03/09/big-data-platform-survey/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Co2y">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Co2y's Blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Co2y's Blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                大数据平台介绍
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-03-09T10:41:08+08:00">
                2017-03-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Big-Data/" itemprop="url" rel="index">
                    <span itemprop="name">Big Data</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/03/09/big-data-platform-survey/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/03/09/big-data-platform-survey/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本文主要对当前主流的大数据处理平台做一个简介，包括它们的发展史、整体结构、相互之间关系和使用场景，属于科普文章。</p>
<a id="more"></a>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>2003年Google发布了Google File System，之后的2004年又发布了MapReduce。受到启发的Doug Cutting等人于2006年开发出Hadoop，同年发表的还有Bigtable（HBase前身）。2008年Hadoop成为Apache顶级项目。早期的Hadoop主要由两个核心组件构成：HDFS和MapReduce编程模型。随着Hadoop的发展，一些围绕在Hadoop周围的开源项目逐步诞生，主要是ZooKeeper、Hive、Pig、HBase、Storm、Kafka、Flume、Sqoop、Oozie、Mahout等，它们共同形成了Hadoop生态圈。2012年，Hadoop V2发布，其中最重要的变化是在Hadoop核心组件中增加了YARN，并把MapReduce作为Hadoop的计算框架。YARN的出现是为了把计算框架与资源管理彻底分离开，解决了Hadoop V1由此带来的扩展性差、单点故障和不能同时支持多种计算框架的问题。YARN对标的是资源调度器，例如Mesos，Borg等。</p>
<p>至此，Hadoop生态圈已经成熟（以下说的Hadoop统一指Hadoop V2），越来越多的处理平台可以统一进Hadoop家族，当今主流的处理平台主要包括Hadoop，HBase，Hive，Storm，Spark，Flink等。</p>
<h2 id="平台"><a href="#平台" class="headerlink" title="平台"></a>平台</h2><h3 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h3><h4 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h4><p>Hadoop的核心包括HDFS和YARN两个部分。HDFS负责底层存储，提供容错，它分为NameNode和DataNode，NameNode上保存着HDFS的名字空间，任何对文件系统元数据产生修改的操作都会作用于NameNode上。DataNode将HDFS数据以块为单位（默认128M）,以文件的形式存储在本地的文件系统中，默认是3备份。因为NameNode保存着所有的元数据，非常重要，所以通常需要一个Secondary NameNode作为替代，并且配置NameNode对它的元数据做持久化备份。</p>
<p><img src="/img/hadoop-hdfs.jpg" alt=""></p>
<p>YARN负责资源的调度和管理，许多开源项目包括下文提到的Storm，Spark和Flink都可以跑在YARN上。将这些框架部署到YARN上可以弹性的分配整个Hadoop集群的计算资源，共享底层存储，在一个统一的平台上完成调度。</p>
<p>YARN包括一个全局的资源管理器ResourceManager，每个节点上的NodeManager和每个应用程序特有的ApplicationMaster。其中ResourceManager负责对各个NodeManager上的资源进行统一管理和调度。当用户提交一个应用程序时，需要提供一个用以跟踪和管理这个程序的ApplicationMaster，它负责向ResourceManager申请资源，并要求NodeManger启动可以占用一定资源的任务。由于不同的ApplicationMaster被分布到不同的节点上，因此它们之间不会相互影响。ApplicationMaster负责单个应用程序的管理。整体结构图如下。</p>
<p><img src="/img/hadoop-yarn.png" alt=""></p>
<h4 id="运算模型"><a href="#运算模型" class="headerlink" title="运算模型"></a>运算模型</h4><p>Hadoop上内置的编程模型是MapReduce，MapReduce作为YARN内置的一种计算框架和编程模型可以执行用户提交在Hadoop集群中的程序。它主要由<code>map()</code>,<code>shuffle()</code>,<code>reduce()</code>几个算子组成。这种简单的编程模型给开发者极大的便利，但是它也有缺点，例如Map之后总是要Reduce，Reduce的结果要存HDFS。</p>
<p><img src="/img/hadoop-mapreduce.png" alt=""></p>
<h4 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h4><p>以WordCount为例，我们需要实现<code>map()</code>和<code>reduce()</code>两个函数，</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> &#123;</span></div><div class="line"></div><div class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">TokenizerMapper</span></span></div><div class="line">       <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Object</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;&#123;</div><div class="line"></div><div class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> IntWritable one = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</div><div class="line">    <span class="keyword">private</span> Text word = <span class="keyword">new</span> Text();</div><div class="line"></div><div class="line">    <span class="keyword">public</span> <span class="keyword">void</span> map(Object key, Text value, Context context</div><div class="line">                    ) <span class="keyword">throws</span> IOException, InterruptedException &#123;</div><div class="line">      StringTokenizer itr = <span class="keyword">new</span> StringTokenizer(value.toString());</div><div class="line">      <span class="keyword">while</span> (itr.hasMoreTokens()) &#123;</div><div class="line">        word.set(itr.nextToken());</div><div class="line">        context.write(word, one);</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">IntSumReducer</span></span></div><div class="line">       <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>,<span class="title">IntWritable</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>&gt; &#123;</div><div class="line">    <span class="keyword">private</span> IntWritable result = <span class="keyword">new</span> IntWritable();</div><div class="line"></div><div class="line">    <span class="keyword">public</span> <span class="keyword">void</span> reduce(Text key, Iterable&lt;IntWritable&gt; values,</div><div class="line">                       Context context</div><div class="line">                       ) <span class="keyword">throws</span> IOException, InterruptedException &#123;</div><div class="line">      <span class="keyword">int</span> sum = <span class="number">0</span>;</div><div class="line">      <span class="keyword">for</span> (IntWritable <span class="string">val :</span> values) &#123;</div><div class="line">        sum += val.get();</div><div class="line">      &#125;</div><div class="line">      result.set(sum);</div><div class="line">      context.write(key, result);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> main(String[] args) <span class="keyword">throws</span> Exception &#123;</div><div class="line">    Configuration conf = <span class="keyword">new</span> Configuration();</div><div class="line">    Job job = Job.getInstance(conf, <span class="string">"word count"</span>);</div><div class="line">    job.setJarByClass(WordCount.<span class="keyword">class</span>);</div><div class="line">    job.setMapperClass(TokenizerMapper.<span class="keyword">class</span>);</div><div class="line">    job.setCombinerClass(IntSumReducer.<span class="keyword">class</span>);</div><div class="line">    job.setReducerClass(IntSumReducer.<span class="keyword">class</span>);</div><div class="line">    job.setOutputKeyClass(Text.<span class="keyword">class</span>);</div><div class="line">    job.setOutputValueClass(IntWritable.<span class="keyword">class</span>);</div><div class="line">    FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</div><div class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</div><div class="line">    System.exit(job.waitForCompletion(<span class="literal">true</span>) ? 0 : <span class="number">1</span>);</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>为了减少中间结果写入磁盘的次数，增强MapReduce的表达能力，改善固有的Map-Reduce—Map-Reduce的编程模式，Hortonworks提出了Tez。Tez是对MapReduce的一层封装，它可以兼容原先的MapReduce，通常也运行在YARN之上。它提供了一组API可以将原先多个有依赖的MapReduce作业转换为一个DAG作业从而大幅提升性能，例如它可以提供Map-Map-Reduce这样的结构。此外Tez通过管理session，避免每次启动MapReduce任务的开销。这两点使得Tez的运行效率通常要高于MapReduce。下面是MapReduce和Tez的一个对比。</p>
<p><img src="/img/hadoop-tez.jpg" alt=""></p>
<h4 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h4><p>正如上面介绍到的，Hadoop包括HDFS和YARN两个部分，其中内置了MapReduce的ApplicationMaster。所以Hadoop的主要场景是为其它组件提供底层存储（HDFS）和资源调度（YARN），而其它组件想要在上面运行需要实现对应的ApplicationMaster。当然，Hadoop也可以跑用户编写的MapReduce程序。</p>
<h3 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h3><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>HBase通常搭建在Hadoop的HDFS之上。HDFS是一个只读的追加型文件系统，它的设计目标是为了存储和分析大量的数据，通常是一次写入多次读取的数据，因此直接修改数据的block代价较高。而HBase的出现正是弥补了Hadoop随机存取的能力。HBase是一个列族型的非关系数据库，里面存的是键值对。HBase是一个LSM树型存储结构，这种设计提高了数据插入的性能，并且在compaction的时候清除被删除的数据，契合HDFS的设计。另外，HBase为了快速读取数据，它的文件块要比HDFS的块小的多。HBase的存储文件称为HFile，HFile是存放在HDFS上的，利用HDFS的容错和可靠性支持。</p>
<p>HBase的逻辑结构如下<br><img src="/img/hbase-logical.png" alt=""></p>
<p>HBase的物理结构如下<br><img src="/img/hbase-physical.png" alt=""></p>
<p>对于开发人员，HBase提供了一组API供客户端来进行CRUD，并且提供了coprocessor的接口，类似于关系型数据库中的触发器和存储过程。</p>
<p>从HBase的表结构可以看出，它只能够根据rowkey进行范围读取，或者进行全表扫描。像索引，join等SQL特性，HBase本身不提供，需要其它的组件提供这些支持。Phoenix作为HBase上的一个SQL层被广泛使用。另外HBase可以和下面提到的Hive集成，以及其它通过Hive获得元数据的SQL引擎配合使用，例如Spark SQL等都可以以SQL的形式来操作HBase上的数据。</p>
<h4 id="代码示例-1"><a href="#代码示例-1" class="headerlink" title="代码示例"></a>代码示例</h4><p>HBase提供了一组API，主要是<code>Put()</code>,<code>Get()</code>,<code>Scan()</code>。</p>
<figure class="highlight gams"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= connection.getTable(TableName.valueOf(TABLE_NAME))</span>;</div><div class="line"><span class="comment">// Write some rows to the table</span></div><div class="line">print(<span class="string">"Write some greetings to the table"</span>);</div><div class="line"><span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; GREETINGS.length; i++) &#123;</div><div class="line">  <span class="comment">// Each row has a unique row key.</span></div><div class="line">  String rowKey = <span class="string">"greeting"</span> + i;</div><div class="line">  <span class="comment">// Put a single row into the table. We could also pass a list of Puts to write a batch.</span></div><div class="line">  <span class="keyword">Put</span> <span class="keyword">put</span> = new <span class="keyword">Put</span>(Bytes.toBytes(rowKey));</div><div class="line">  <span class="keyword">put</span>.addColumn(COLUMN_FAMILY_NAME, COLUMN_NAME, Bytes.toBytes(GREETINGS[i]));</div><div class="line">  <span class="keyword">table</span>.put(put);</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">String rowKey = <span class="string">"greeting0"</span>;</div><div class="line">Result getResult = <span class="keyword">table</span>.get(new <span class="comment">Get(Bytes.toBytes(rowKey)))</span>;</div><div class="line">String greeting = Bytes.toString(getResult.getValue(COLUMN_FAMILY_NAME, COLUMN_NAME));</div><div class="line"></div><div class="line"></div><div class="line">Scan scan = new Scan();</div><div class="line">print(<span class="string">"Scan for all greetings:"</span>);</div><div class="line">ResultScanner scanner = <span class="keyword">table</span>.getScanner(scan);</div><div class="line">for (Result row : scanner) &#123;</div><div class="line">  byte[] valueBytes = row.getValue(COLUMN_FAMILY_NAME, COLUMN_NAME);</div><div class="line">  <span class="keyword">System</span>.out.println(<span class="string">'\t'</span> + Bytes.toString(valueBytes));</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h4><p>HBase建立在HDFS之上，是一个列族型的Key-Value数据库，提供键值存储，支持随机查询和修改，支持范围查询，面向OLTP，提供行级的一致性。但是因为HBase查询单一，所以除非业务完全适合HBase模型，一般需要和Phoenix或者其它OLAP工具结合使用。</p>
<h3 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h3><h4 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h4><p>Hive的出现是为了解决数据分析人员需要编写大量MapReduce任务的问题。它能够将SQL翻译成对应的MapReduce job。Hive提供了CLI，JDBC，Web等多种客户端接口。Hive既可以操作HDFS上的数据，也可以操作HBase里的数据，除了顺序扫描，Hive都是以MapReduce的方式去进行数据分析和处理的。</p>
<p><img src="/img/hive-cluster.png" alt=""></p>
<p>值得一提的是，虽然Hive内置的计算引擎是MapReduce，但它也可以和上面提到的Tez配合使用，Tez能够运行任意MR任务且不需要改动。此外Hive可以为其他SQL on Hadoop方案提供元数据，例如后面提到的Spark SQL，以及Impala等，从这个角度来看，Hive是一个中间者的角色，负责提供元数据，具体的计算由其它引擎完成。</p>
<h4 id="代码示例-2"><a href="#代码示例-2" class="headerlink" title="代码示例"></a>代码示例</h4><p>Hive可以通过JDBC的方式操作数据。</p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">try &#123;</div><div class="line">  Class.forName(driverName);</div><div class="line">&#125; catch (ClassNotFoundException e) &#123;</div><div class="line">  <span class="regexp">//</span> TODO Auto-generated catch block</div><div class="line">  e.printStackTrace();</div><div class="line">  System.<span class="keyword">exit</span>(<span class="number">1</span>);</div><div class="line">&#125;</div><div class="line">Connection con = DriverManager.getConnection(<span class="string">"jdbc:hive://localhost:10000/default"</span>, <span class="string">""</span>, <span class="string">""</span>);</div><div class="line">Statement stmt = con.createStatement();</div><div class="line">String tableName = <span class="string">"testHiveDriverTable"</span>;</div><div class="line">stmt.executeQuery(<span class="string">"drop table "</span> + tableName);</div><div class="line">ResultSet res = stmt.executeQuery(<span class="string">"create table "</span> + tableName + <span class="string">" (key int, value string)"</span>);</div><div class="line"><span class="regexp">//</span> show tables</div><div class="line">String sql = <span class="string">"show tables '"</span> + tableName + <span class="string">"'"</span>;</div><div class="line">System.out.println(<span class="string">"Running: "</span> + sql);</div><div class="line">res = stmt.executeQuery(sql);</div><div class="line"><span class="keyword">if</span> (res.<span class="keyword">next</span>()) &#123;</div><div class="line">  System.out.println(res.getString(<span class="number">1</span>));</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="使用场景-1"><a href="#使用场景-1" class="headerlink" title="使用场景"></a>使用场景</h4><p>Hive在HDFS之上提供数据分析能力，以SQL的方式操作数据，免去数据分析人员频繁编写简单MapReduce程序的工作。通常适用于OLAP场景，如大批量数据的查询和分析，ETL，生成报表等。Hive的SQL兼容SQL 92，用户可以通过JDBC的方式连接Hive，操作数据。</p>
<p>此外，Hive为Spark SQL、Flink等其它SQL on Hadoop工具提供元数据。</p>
<h3 id="Storm"><a href="#Storm" class="headerlink" title="Storm"></a>Storm</h3><h4 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h4><p>不同于Hive这种先把数据放入HDFS再进行处理的批处理方式，Storm提供了实时的流式处理，两者互补分别解决不同场景下的问题。将Storm部署到Hadoop之上，可以和其它程序（如MapReduce）共享整个集群的资源，通过YARN统一管理。</p>
<p>Storm实现了一个数据流(data flow)的模型，在这个模型中数据持续不断地流经一个由很多转换实体构成的网络。一个数据流的抽象叫做流(stream)，流是无限的元组(Tuple)序列。元组就像一个可以表示标准数据类型（例如int，float和byte数组）和用户自定义类型（需要额外序列化代码的）的数据结构。Storm拓扑图如下。</p>
<p><img src="/img/storm-structure.png" alt=""></p>
<p>Storm中的流的来源叫Spout，通常Spout从外部数据源读取数据，例如kafka，然后吐到拓扑里。在拓扑中所有的计算逻辑都是在Bolt中实现的。一个Bolt可以处理任意数量的输入流，产生任意数量新的输出流。Bolt可以做函数处理，过滤，流的合并，聚合，存储到数据库等操作。</p>
<h4 id="代码示例-3"><a href="#代码示例-3" class="headerlink" title="代码示例"></a>代码示例</h4><p>以下是流式处理WordCount。</p>
<figure class="highlight arduino"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">TopologyBuilder builder = <span class="keyword">new</span> TopologyBuilder();</div><div class="line">builder.setSpout(<span class="string">"spout"</span>, <span class="keyword">new</span> RandomSentenceSpout(), <span class="number">5</span>);</div><div class="line">builder.setBolt(<span class="string">"split"</span>, <span class="keyword">new</span> Split(), <span class="number">8</span>).shuffleGrouping(<span class="string">"spout"</span>);</div><div class="line">builder.setBolt(<span class="string">"count"</span>, <span class="keyword">new</span> WordCount(), <span class="number">12</span>).fieldsGrouping(<span class="string">"split"</span>, <span class="keyword">new</span> Fields(<span class="string">"word"</span>));</div><div class="line"></div><div class="line">...</div><div class="line"></div><div class="line">Map&lt;<span class="keyword">String</span>, Integer&gt; counts = <span class="keyword">new</span> HashMap&lt;<span class="keyword">String</span>, Integer&gt;();</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">void</span> execute(Tuple tuple, BasicOutputCollector collector) &#123;</div><div class="line">  <span class="keyword">String</span> <span class="keyword">word</span> = tuple.getString(<span class="number">0</span>);</div><div class="line">  Integer count = counts.containsKey(<span class="keyword">word</span>) ? counts.<span class="built_in">get</span>(<span class="keyword">word</span>) + <span class="number">1</span> : <span class="number">1</span>;</div><div class="line">  counts.<span class="built_in">put</span>(<span class="keyword">word</span>, count);</div><div class="line">  collector.emit(<span class="keyword">new</span> Values(<span class="keyword">word</span>, count));</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="使用场景-2"><a href="#使用场景-2" class="headerlink" title="使用场景"></a>使用场景</h4><p>强实时性，流式处理。如实时分析日志和入库。</p>
<h3 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h3><h4 id="简介-3"><a href="#简介-3" class="headerlink" title="简介"></a>简介</h4><p>Spark官方定义是一个用于大规模数据处理的快速和通用引擎。它和Hadoop的结合主要表现在它可以利用YARN进行资源调度和管理，利用HDFS作为持久化存储，当然这两者都是可以替换的。Spark已经逐渐发展为一个完整的数据平台，上面提供了多种处理方式，包括Spark SQL，Spark Streaming，图计算，机器学习库等。</p>
<p>Spark集群也是一个主从式架构。</p>
<p><img src="/img/spark-deploy.png" alt=""></p>
<p>Spark将数据抽象成RDD（是弹性分布式数据集），它是一个容错的、并行的数据结构，可以让用户显式地将数据存储到磁盘和内存中，并能控制数据的分区。RDD有两类操作Transformation和Action，前者只记录转换规则，后者触发真正的操作。因此可以通过Transformation的“世系”机制来对RDD进行恢复做容错处理。</p>
<p>RDD的一个Action对应一个job。整个job会产生哪些RDD由transformation来决定，一个RDD包含多个partition，每个分区对应Executor中的一个task线程。RDD之间有NarrowDependency和ShuffleDependency(wide)两种，只有前后两个RDD的partition个数及partitioner都一样才会出现NarrowDependency，这也是划分一个job中stage位置的判断依据。</p>
<p>Spark中的API都是建立在RDD之上的，包括处理二维表的DataFrame和处理流的DStream。</p>
<h4 id="代码示例-4"><a href="#代码示例-4" class="headerlink" title="代码示例"></a>代码示例</h4><p>Spark在RDD的基础上提供了DataSet，DataFrame和DStream，可以方便的应对各种处理需求。</p>
<figure class="highlight arduino"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line">Dataset&lt;Row&gt; df = spark.<span class="built_in">read</span>().json(<span class="string">"examples/src/main/resources/people.json"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Register the DataFrame as a SQL temporary view</span></div><div class="line">df.createOrReplaceTempView(<span class="string">"people"</span>);</div><div class="line"></div><div class="line">Dataset&lt;Row&gt; sqlDF = spark.sql(<span class="string">"SELECT * FROM people"</span>);</div><div class="line">sqlDF.show();</div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// | age|   name|</span></div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// |null|Michael|</span></div><div class="line"><span class="comment">// |  30|   Andy|</span></div><div class="line"><span class="comment">// |  19| Justin|</span></div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">// Creates a DataFrame based on a table named "people"</span></div><div class="line"><span class="comment">// stored in a MySQL database.</span></div><div class="line"><span class="keyword">String</span> url =</div><div class="line">  <span class="string">"jdbc:mysql://yourIP:yourPort/test?user=yourUsername;password=yourPassword"</span>;</div><div class="line">DataFrame df = sqlContext</div><div class="line">  .<span class="built_in">read</span>()</div><div class="line">  .format(<span class="string">"jdbc"</span>)</div><div class="line">  .option(<span class="string">"url"</span>, url)</div><div class="line">  .option(<span class="string">"dbtable"</span>, <span class="string">"people"</span>)</div><div class="line">  .load();</div><div class="line"><span class="comment">// Looks the schema of this DataFrame.</span></div><div class="line">df.printSchema();</div><div class="line"><span class="comment">// Counts people by age</span></div><div class="line">DataFrame countsByAge = df.groupBy(<span class="string">"age"</span>).count();</div><div class="line">countsByAge.show();</div><div class="line"><span class="comment">// Saves countsByAge to S3 in the JSON format.</span></div><div class="line">countsByAge.<span class="built_in">write</span>().format(<span class="string">"json"</span>).save(<span class="string">"s3a://..."</span>);</div><div class="line"></div><div class="line"></div><div class="line">StreamingExamples.setStreamingLogLevels();</div><div class="line">SparkConf sparkConf = <span class="keyword">new</span> SparkConf().setAppName(<span class="string">"JavaKafkaWordCount"</span>);</div><div class="line"><span class="comment">// Create the context with 2 seconds batch size</span></div><div class="line">JavaStreamingContext jssc = <span class="keyword">new</span> JavaStreamingContext(sparkConf, <span class="keyword">new</span> Duration(<span class="number">2000</span>));</div><div class="line"><span class="keyword">int</span> numThreads = Integer.<span class="built_in">parseInt</span>(args[<span class="number">3</span>]);</div><div class="line">Map&lt;<span class="keyword">String</span>, Integer&gt; topicMap = <span class="keyword">new</span> HashMap&lt;&gt;();</div><div class="line"><span class="keyword">String</span>[] topics = args[<span class="number">2</span>].split(<span class="string">","</span>);</div><div class="line"><span class="built_in">for</span> (<span class="keyword">String</span> topic: topics) &#123;</div><div class="line">  topicMap.<span class="built_in">put</span>(topic, numThreads);</div><div class="line">&#125;</div><div class="line">JavaPairReceiverInputDStream&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; messages =</div><div class="line">        KafkaUtils.createStream(jssc, args[<span class="number">0</span>], args[<span class="number">1</span>], topicMap);</div><div class="line">JavaDStream&lt;<span class="keyword">String</span>&gt; lines = messages.<span class="built_in">map</span>(Tuple2::_2);</div><div class="line">JavaDStream&lt;<span class="keyword">String</span>&gt; words = lines.flatMap(x -&gt; Arrays.asList(SPACE.split(x)).iterator());</div><div class="line">JavaPairDStream&lt;<span class="keyword">String</span>, Integer&gt; wordCounts = words.mapToPair(s -&gt; <span class="keyword">new</span> Tuple2&lt;&gt;(s, <span class="number">1</span>))</div><div class="line">    .reduceByKey((i1, i2) -&gt; i1 + i2);</div><div class="line">wordCounts.<span class="built_in">print</span>();</div><div class="line">jssc.start();</div><div class="line">jssc.awaitTermination();</div></pre></td></tr></table></figure>
<h4 id="使用场景-3"><a href="#使用场景-3" class="headerlink" title="使用场景"></a>使用场景</h4><p>完整的处理引擎，可以和Hadoop，Hive，HBase配合使用。从数据处理逻辑的角度来看，MapReduce相当于Spark中的<code>map()</code>+<code>reduceByKey()</code>，此外Spark还提供了更加丰富的算子可以用于流处理和SQL处理。由于RDD的特性，Spark不适合细粒度状态更新的应用，例如Spark Streaming是通过微批处理的方式来进行流式计算的。</p>
<h3 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h3><h4 id="简介-4"><a href="#简介-4" class="headerlink" title="简介"></a>简介</h4><p>Flink和Hadoop的结合也是通过HDFS和YARN。默认情况下Flink的Standalone有自己的JobManager和TaskManager。甚至于在YARN中，Flink也承担了一部分任务管理的功能。</p>
<p><img src="/img/FlinkOnYarn.svg" alt=""></p>
<p>要介绍Flink，需要把它和Spark对比其他看。早期的Flink是一个流式处理平台，随着发展，它和Spark越来越像。Flink最大的特点是把所有的抽象都建立在流之上，Spark中的流处理是在一批RDD之上，把流处理看做是特殊的批处理，而Flink则是把批处理看成Streaming的特殊例子，差异如下：</p>
<p>其一，在实时计算问题上，Flink提供了基于每个事件的流式处理机制，所以它可以被认为是一个真正意义上的流式计算，类似于Storm的计算模型。而对于Spark来说，不是基于事件粒度的，而是用小批量来模拟流式，也就是多个事件的集合。所以，Spark被认为是一个接近实时的处理系统。虽然，大部分应用实时是可以接受的，但对于很多应用需要基于事件级别的流式计算，因而会选择Storm而不是Spark Streaming，现在Flink也许是一个不错的选择。</p>
<h4 id="代码示例-5"><a href="#代码示例-5" class="headerlink" title="代码示例"></a>代码示例</h4><p>还是以WordCount为例。</p>
<figure class="highlight processing"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</div><div class="line"><span class="comment">// get input data</span></div><div class="line">DataStream&lt;<span class="keyword">String</span>&gt; <span class="built_in">text</span> = getTextDataStream(env);</div><div class="line">DataStream&lt;Tuple2&lt;<span class="keyword">String</span>, Integer&gt;&gt; counts =</div><div class="line">        <span class="comment">// normalize and split each line</span></div><div class="line">        <span class="built_in">text</span>.<span class="built_in">map</span>(<span class="built_in">line</span> -&gt; <span class="built_in">line</span>.toLowerCase().<span class="built_in">split</span>(<span class="string">"\\W+"</span>))</div><div class="line">        <span class="comment">// convert splitted line in pairs (2-tuples) containing: (word,1)</span></div><div class="line">        .flatMap((<span class="keyword">String</span>[] tokens, Collector&lt;Tuple2&lt;<span class="keyword">String</span>, Integer&gt;&gt; out) -&gt; &#123;</div><div class="line">            <span class="comment">// emit the pairs with non-zero-length words</span></div><div class="line">            Arrays.stream(tokens)</div><div class="line">            .<span class="built_in">filter</span>(t -&gt; t.length() &gt; <span class="number">0</span>)</div><div class="line">            .forEach(t -&gt; out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(t, <span class="number">1</span>)));</div><div class="line">        &#125;)</div><div class="line">        <span class="comment">// group by the tuple field "0" and sum up tuple field "1"</span></div><div class="line">        .keyBy(<span class="number">0</span>)</div><div class="line">        .sum(<span class="number">1</span>);</div><div class="line"><span class="comment">// emit result</span></div><div class="line"><span class="keyword">if</span>(fileOutput) &#123;</div><div class="line">    counts.writeAsCsv(outputPath);</div><div class="line">&#125; <span class="keyword">else</span> &#123;</div><div class="line">    counts.<span class="built_in">print</span>();</div><div class="line">&#125;</div><div class="line"><span class="comment">// execute program</span></div><div class="line">env.execute(<span class="string">"Streaming WordCount Example"</span>);</div></pre></td></tr></table></figure>
<h4 id="使用场景-4"><a href="#使用场景-4" class="headerlink" title="使用场景"></a>使用场景</h4><p>Flink的发展路线同Spark一样，也是往完整的生态系统发展。目前Flink上的组件也支持关系型表（SQL），图处理，机器学习库，另外它支持实时响应的流式处理。但是Flink整体成熟度不如Spark。</p>
<h2 id="平台小结"><a href="#平台小结" class="headerlink" title="平台小结"></a>平台小结</h2><p>自Hadoop V2提供批处理MR编程模型以来，为了减轻数据处理人员编写MapReduce任务的麻烦，Hive出现了，它将人们熟悉的SQL翻译成MapReduce job，使得数据分析人员可以通过SQL来分析数据。为了解决Hive/Hadoop文件只读的问题，HBase作为一个NoSQL数据库提供了快速地随机读取、修改数据的能力，并且提供OLTP，保证行级的事务性，结合Tephra可以获得完整的ACID，并且可以和Hive以及其它SQL on Hadoop工具配合使用。为了解决固有MapReduce编程模型的缺点，以及减少迭代式任务中间数据存取硬盘的开销，Tez、Impala、Spark SQL等SQL on Hadoop组件出现了，它们利用Hive提供的元数据，优化原先的MR固定模型，提供更加细粒度的算子，以DAG的形式运行整个job，因此能够更快的获得SQL结果。为了进行实时处理，Storm，Spark Streaming，Flink出现了。最后，为了统一批处理和流处理，Spark和Flink作为主流的两个平台正在往通用计算引擎方向发展。而在这之上，还有统一Spark和Flink等平台API的Apache Beam项目。</p>
<p>目前的企业架构上通常是把Hadoop、Hive、HBase、Spark和Flink结合使用。为了方便搭建这些平台，在社区版本的基础上，还有一些Hadoop厂商提供企业发行版，例如CDH，HDP等。这些企业Hadoop发行版将上述提到的开源组件整合到了一个平台之上（还有Hadoop生态圈的其它组件）并做了一些定制，并且提供了安装、部署、监控的工具，大大方便了平台运维人员。</p>
<h2 id="数据处理库"><a href="#数据处理库" class="headerlink" title="数据处理库"></a>数据处理库</h2><h3 id="Apache-Mahout"><a href="#Apache-Mahout" class="headerlink" title="Apache Mahout"></a>Apache Mahout</h3><p>Mahout提供了适用于Hadoop的机器学习和数据挖掘的分布式计算框架。早期的Mahout基于MapReduce，但是MapReduce本身不适用与迭代式计算，近年来随着Mahout Samsara的发展，现在的Mahout可以和Spark、H2O、Flink等多种计算引擎结合使用，原先MapReduce的计算方式也已经被抛弃了。新生的Samsara提供了Scala和Shell的接口，供用户使用。</p>
<p>Mahout提供了许多现有的算法库，包括但不限于以下这些：</p>
<ul>
<li>协同过滤<ol>
<li>基于用户</li>
<li>基于物品</li>
<li>ALS、SVD矩阵分解等</li>
</ol>
</li>
<li>分类<ol>
<li>逻辑回归</li>
<li>贝叶斯</li>
<li>隐式马尔科夫等</li>
</ol>
</li>
<li>聚类<ol>
<li>K-Means</li>
<li>谱聚类等</li>
</ol>
</li>
<li>降维<ol>
<li>SVD</li>
<li>PCA等</li>
</ol>
</li>
<li>主题模型<ol>
<li>LDA</li>
</ol>
</li>
<li>杂项</li>
</ul>
<p>Mahout的主要应用场景是推荐引擎，许多商业用例可以在<a href="https://mahout.apache.org/general/powered-by-mahout.html" target="_blank" rel="external">这里</a>看到。</p>
<blockquote>
<p>Mahout为数据分析人员，解决了大数据的门槛；为算法工程师，提供基础的算法库；为Hadoop开发人员，提供了数据建模的标准；为运维人员，打通了和Hadoop连接。</p>
</blockquote>
<h3 id="Spark-MLlib"><a href="#Spark-MLlib" class="headerlink" title="Spark MLlib"></a>Spark MLlib</h3><p>MLlib是Spark中提供机器学习的库，原先建立在RDD之上，现在的ML也提供了DataFrame之上的API。MLlib也实现了许多经典的算法，并且提供了Scala、Java、Python等接口，具体的算法包括但不限于：</p>
<ul>
<li>基本统计<ol>
<li>概况统计</li>
<li>相关性</li>
<li>分层抽样</li>
<li>假设检验</li>
<li>随机数生成</li>
</ol>
</li>
<li>分类和回归<ol>
<li>线性模型（SVM，线性回归，逻辑回归）</li>
<li>贝叶斯</li>
<li>决策树</li>
<li>随机森林等</li>
</ol>
</li>
<li>协同过滤<ol>
<li>ALS</li>
</ol>
</li>
<li>聚类<ol>
<li>KMeans</li>
</ol>
</li>
<li>降维<ol>
<li>PCA</li>
<li>SVD</li>
</ol>
</li>
<li>优化部分<ol>
<li>随机梯度下降等</li>
</ol>
</li>
</ul>
<h3 id="使用方式"><a href="#使用方式" class="headerlink" title="使用方式"></a>使用方式</h3><p>要想使用这些机器学习库其实很方便，只要提供符合InputFormat的数据集，就可以调用对应的算法。下面是两个例子：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">// <span class="keyword">Load</span> training <span class="keyword">data</span> <span class="keyword">in</span> LIBSVM format.</div><div class="line">val <span class="keyword">data</span> = MLUtils.loadLibSVMFile(sc, <span class="string">"data/mllib/sample_libsvm_data.txt"</span>)</div><div class="line">// <span class="keyword">Split</span> <span class="keyword">data</span> <span class="keyword">into</span> training (<span class="number">60</span>%) <span class="keyword">and</span> <span class="keyword">test</span> (<span class="number">40</span>%).</div><div class="line">val splits = data.randomSplit(<span class="built_in">Array</span>(<span class="number">0.6</span>, <span class="number">0.4</span>), <span class="keyword">seed</span> = <span class="number">11</span>L)</div><div class="line">val training = splits(<span class="number">0</span>).cache()</div><div class="line">val <span class="keyword">test</span> = splits(<span class="number">1</span>)</div><div class="line">// Run training algorithm <span class="keyword">to</span> <span class="keyword">build</span> the <span class="keyword">model</span></div><div class="line">val numIterations = <span class="number">100</span></div><div class="line">val <span class="keyword">model</span> = SVMWithSGD.train(training, numIterations)</div><div class="line">// <span class="keyword">Clear</span> the <span class="keyword">default</span> threshold.</div><div class="line">model.clearThreshold()</div><div class="line">// <span class="keyword">Compute</span> <span class="keyword">raw</span> scores <span class="keyword">on</span> the <span class="keyword">test</span> set.</div><div class="line">val scoreAndLabels = test.map &#123; point =&gt;</div><div class="line">    val score = model.predict(point.features)</div><div class="line">    (score, point.label)</div><div class="line">&#125;</div><div class="line">// <span class="keyword">Get</span> evaluation metrics.</div><div class="line">val metrics = <span class="keyword">new</span> BinaryClassificationMetrics(scoreAndLabels)</div><div class="line">val auROC = metrics.areaUnderROC()</div><div class="line">println(<span class="string">"Area under ROC = "</span> + auROC)</div></pre></td></tr></table></figure>
<figure class="highlight pony"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Every record of this DataFrame contains the label and</span></div><div class="line"><span class="comment">// features represented by a vector.</span></div><div class="line"><span class="meta">val</span> df = sqlContext.createDataFrame(data).toDF(<span class="string">"label"</span>, <span class="string">"features"</span>)</div><div class="line"><span class="comment">// Set parameters for the algorithm.</span></div><div class="line"><span class="comment">// Here, we limit the number of iterations to 10.</span></div><div class="line"><span class="meta">val</span> lr = <span class="function"><span class="keyword">new</span> <span class="title">LogisticRegression</span>().<span class="title">setMaxIter</span>(<span class="number">10</span>)</span></div><div class="line"><span class="comment">// Fit the model to the data.</span></div><div class="line"><span class="title">val</span> <span class="title">model</span> = <span class="title">lr</span>.<span class="title">fit</span>(df)</div><div class="line"><span class="comment">// Inspect the model: get the feature weights.</span></div><div class="line"><span class="title">val</span> <span class="title">weights</span> = <span class="title">model</span>.<span class="title">weights</span></div><div class="line"><span class="comment">// Given a dataset, predict each point's label, and show the results.</span></div><div class="line"><span class="title">model</span>.<span class="title">transform</span>(df).<span class="title">show</span>()</div></pre></td></tr></table></figure>
<h3 id="使用场景-5"><a href="#使用场景-5" class="headerlink" title="使用场景"></a>使用场景</h3><p>许多商业场景都可以使用上述的经典算法，例如市场营销和风险管理等。</p>
<ul>
<li>营销响应分析建模（逻辑回归，决策树）</li>
<li>净提升度分析建模（关联规则）</li>
<li>客户保有分析建模（卡普兰梅尔分析，神经网络）</li>
<li>购物篮分析（关联分析Apriori）</li>
<li>自动推荐系统（协同过滤推荐，基于内容推荐，基于人口统计推荐，基于知识推荐，组合推荐，关联规则）</li>
<li>客户细分（聚类）</li>
<li>流失预测（逻辑回归）</li>
<li>客户信用风险评分（SVM，决策树，神经网络）</li>
<li>市场风险评分建模（逻辑回归和决策树）</li>
<li>运营风险评分建模（SVM）</li>
<li>欺诈检测（决策树，聚类，社交网络）</li>
</ul>
<h2 id="更多阅读"><a href="#更多阅读" class="headerlink" title="更多阅读"></a>更多阅读</h2><ul>
<li><a href="http://blog.co2y.me/2016/02/29/something-about-hive/" target="_blank" rel="external">hive SQL转换过程</a></li>
<li><a href="http://blog.co2y.me/2016/09/22/phoenix-executequery-readingnotes/" target="_blank" rel="external">Phoenix SQL执行过程</a></li>
<li><a href="http://blog.co2y.me/2017/02/24/spark-vs-flink/" target="_blank" rel="external">Spark vs Flink</a></li>
<li><a href="http://blog.co2y.me/2016/07/26/hbase-sourcecode-reading-1/" target="_blank" rel="external">HBase读取与写入</a></li>
<li><a href="http://blog.co2y.me/2015/11/15/hbase-brief/" target="_blank" rel="external">HBase基本结构</a></li>
</ul>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/big-data/" rel="tag"># big data</a>
          
            <a href="/tags/survey/" rel="tag"># survey</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/03/05/hihocoder-offer-8/" rel="next" title="hihocoder-Offer收割编程练习赛8">
                <i class="fa fa-chevron-left"></i> hihocoder-Offer收割编程练习赛8
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/04/14/spiral-matrix/" rel="prev" title="spiral matrix中任意位置的元素">
                spiral matrix中任意位置的元素 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

          
          </div>
          


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2017/03/09/big-data-platform-survey/"
           data-title="大数据平台介绍" data-url="http://co2y.github.io/2017/03/09/big-data-platform-survey/">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Co2y" />
          <p class="site-author-name" itemprop="name">Co2y</p>
           
              <p class="site-description motion-element" itemprop="description">think, record, do</p>
          
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">42</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">15</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">47</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/co2y" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/u/2408622401" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#背景"><span class="nav-number">1.</span> <span class="nav-text">背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#平台"><span class="nav-number">2.</span> <span class="nav-text">平台</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Hadoop"><span class="nav-number">2.1.</span> <span class="nav-text">Hadoop</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基本结构"><span class="nav-number">2.1.1.</span> <span class="nav-text">基本结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#运算模型"><span class="nav-number">2.1.2.</span> <span class="nav-text">运算模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#代码示例"><span class="nav-number">2.1.3.</span> <span class="nav-text">代码示例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用场景"><span class="nav-number">2.1.4.</span> <span class="nav-text">使用场景</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HBase"><span class="nav-number">2.2.</span> <span class="nav-text">HBase</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#简介"><span class="nav-number">2.2.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#代码示例-1"><span class="nav-number">2.2.2.</span> <span class="nav-text">代码示例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#适用场景"><span class="nav-number">2.2.3.</span> <span class="nav-text">适用场景</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hive"><span class="nav-number">2.3.</span> <span class="nav-text">Hive</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#简介-1"><span class="nav-number">2.3.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#代码示例-2"><span class="nav-number">2.3.2.</span> <span class="nav-text">代码示例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用场景-1"><span class="nav-number">2.3.3.</span> <span class="nav-text">使用场景</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Storm"><span class="nav-number">2.4.</span> <span class="nav-text">Storm</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#简介-2"><span class="nav-number">2.4.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#代码示例-3"><span class="nav-number">2.4.2.</span> <span class="nav-text">代码示例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用场景-2"><span class="nav-number">2.4.3.</span> <span class="nav-text">使用场景</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark"><span class="nav-number">2.5.</span> <span class="nav-text">Spark</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#简介-3"><span class="nav-number">2.5.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#代码示例-4"><span class="nav-number">2.5.2.</span> <span class="nav-text">代码示例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用场景-3"><span class="nav-number">2.5.3.</span> <span class="nav-text">使用场景</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink"><span class="nav-number">2.6.</span> <span class="nav-text">Flink</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#简介-4"><span class="nav-number">2.6.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#代码示例-5"><span class="nav-number">2.6.2.</span> <span class="nav-text">代码示例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用场景-4"><span class="nav-number">2.6.3.</span> <span class="nav-text">使用场景</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#平台小结"><span class="nav-number">3.</span> <span class="nav-text">平台小结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据处理库"><span class="nav-number">4.</span> <span class="nav-text">数据处理库</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Apache-Mahout"><span class="nav-number">4.1.</span> <span class="nav-text">Apache Mahout</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-MLlib"><span class="nav-number">4.2.</span> <span class="nav-text">Spark MLlib</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用方式"><span class="nav-number">4.3.</span> <span class="nav-text">使用方式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用场景-5"><span class="nav-number">4.4.</span> <span class="nav-text">使用场景</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#更多阅读"><span class="nav-number">5.</span> <span class="nav-text">更多阅读</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Co2y</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    
    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"co2y"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    
    <script src="/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  













  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

  


  

</body>
</html>
